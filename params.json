{"name":"Validatar","tagline":"Functional testing framework for Big Data pipelines","body":"# Validatar\r\n\r\n[![Build Status](https://travis-ci.org/yahoo/validatar.svg?branch=master)](https://travis-ci.org/yahoo/validatar) [![Coverage Status](https://coveralls.io/repos/yahoo/validatar/badge.svg?branch=master)](https://coveralls.io/r/yahoo/validatar?branch=master) [![Download](https://api.bintray.com/packages/yahoo/maven/validatar/images/download.svg)](https://bintray.com/yahoo/maven/validatar/_latestVersion)\r\n\r\n... is a Functional Testing Framework for Big Data pipelines. We currently support querying data through Hive (HiveServer2) and Pig (PigServer). Since a lot of other datasources (e.g. Storm DRPC) expose a REST interface, a REST datasource is also supported. You can GET or POST to your endpoint and parse the result into a standard format using some custom Javascript. Validatar is currently compiled against *Pig-0.14*. Running against an older or newer version may result in issues if interfaces have changed. These are relatively minor from experience and can be fixed with relatively minor fixes to engine code if absolutely needed.\r\n\r\n## How to build Validatar\r\n\r\nYou need maven/JDK (1.8.60+ for Nashorn) to build Validatar.\r\n\r\nRun:\r\n\r\n    make jar\r\n\r\n## Writing Tests\r\n\r\n### Test file format\r\n\r\nTest files are written in the YAML format. See examples of all different datasources in src/test/resources/. The schema is as follows:\r\n\r\n```\r\nname: Test family name : String\r\ndescription: Test family description : String\r\nqueries:\r\n   - name: Query name : String : Ex \"Analytics\"\r\n     engine: Execution engine : String Ex \"hive\" or \"pig\" or \"rest\"\r\n     value: Query : String : Ex \"SELECT COUNT(*) AS pv_count FROM page_data\"\r\n   ...\r\ntests:\r\n   - name: Test name : String\r\n     description: Test description : String\r\n     asserts:\r\n        - Assertion on some query. Query name is prefixed to the value. : Ex Analytics.pv_count > 10000\r\n   ...\r\n```\r\n\r\nQueries are named, this name is used as a namespace for all the values returned from the query. In the above example, we created a query named \"Analytics\". It stores the return value \"pv_count\". We are then able to use this in our later asserts.\r\n\r\nValidatar can run a single test file or a folder of test files. Use the --help option to see more details or refer to the Help section below.\r\n\r\n### Assertions\r\n\r\nAssertions are quite flexible, allowing for the following operations:\r\n\r\n```\r\n                   >  : greater than\r\n                   >= : greater or equal to\r\n                   <  : less than\r\n                   <= : less or equal to\r\n                   == : equal to\r\n                   != : not equal to\r\n                   +  : add\r\n                   -  : subtract\r\n                   *  : multiply\r\n                   /  : divide\r\n                   && : boolean and\r\n                   || : boolean or\r\napprox(a, b, percent) : true if a and b within percent difference (0.0 to 1.0) of each other.\r\n```\r\n\r\nSince these are the only operations we currently support, assertions are scalar, i.e. they operate on a single row. As a result, *Validatar currently limits all results from running queries to a single row.* Any need for aggregate or multi-row consideration can be pushed into the query itself. Please do open issues if you find that you are unable to do so and we will look into relaxing this requirement.\r\n\r\n### Parameter Substitution\r\n\r\nYou may want queries that use a specific date column, or similar changing parameter. For this, we have a parameter substation feature.\r\n\r\nSimply pass `--parameter KEY=VALUE` in the CLI and the `KEY` will be replaced with `VALUE` in all queries. For example, to query June 23rd 2015, you could use `--parameter DATE=2015-06-23`. If the query uses `${DATE}` in the query it will be replaced before execution with `2015-06-23`.\r\n\r\n### Execution Engines\r\n\r\n#### Hive\r\n\r\nThe query part of a Hive test is just a HiveSQL statement. We recommend that you push all the heavy lifting to the query - joins, aggregate results etc. We use Hive JDBC underneath to execute against HiveServer2 and fetch the results. We support hive settings at the execution level by passing in --hive-setting arguments to validatar.\r\n\r\nSome mock tests can be found in src/test/resources/sample-tests/tests.yaml\r\n\r\n#### Pig\r\n\r\nThe query part of a Pig test is a PigLatin script. You can register your UDFs etc as long as you register them with the full path to them at runtime. We use PigServer underneath to run the query. You can provide the alias in the script to fetch your results from (or leave it to the default). Setting the exec mode and other pig settings are supported.\r\n\r\nSome mock tests can be found in src/test/resources/pig-tests/sample.yaml\r\n\r\n#### REST\r\n\r\nThe query part of a REST test is a Javascript function that processes the response from your HTTP endpoint into a standard table-like format - a Javascript object (dictionary) where the keys are the column names and the value is an array of the column values.\r\nWe execute the native Javascript via Nashorn. The function that takes a single argument - the string response from your endpoint. The name of this function is customizable if desired.\r\n\r\nThe metadata for the query is used to define the REST call. We currently support setting the method (defaults to GET), the body (if POST), timeout, retry and custom headers.\r\n\r\nThis execution engine exists essentially a catch-all for any other type of Big-Data datasource that has a REST interface but is not natively in Validatar.\r\n\r\nSome mock tests and examples can be found in src/test/resources/rest-tests/sample.yaml\r\n\r\n## How to run\r\n\r\nUse hadoop jar validatar-jar-with-dependencies.jar com.yahoo.validatar.App --help (or -h) for Help\r\n\r\n### To run Hive tests in Validatar:\r\n\r\n    export HADOOP_CLASSPATH=\"$HADOOP_CLASSPATH:/path/to/hive/jdbc/lib/jars/*\"\r\n    hadoop jar validatar-jar-with-dependencies.jar com.yahoo.validatar.App -s tests/ --report report.xml --hive-jdbc ...\r\n\r\nHive needs the JDBC uri of HiveServer2. Note that the DB is in the URI.\r\n\r\n```\r\n--hive-jdbc \"jdbc:hive2://<URI>/<DB>;<Optional params: E.g. sasl.qop=auth;principal=hive/<PRINCIPAL_URL> etc>\r\n```\r\n\r\nDo not add it if your queries use the\r\n\r\n```\r\n... FROM DB.TABLE WHERE ...\r\n```\r\n\r\nformat. Instead, you should leave it out and have ALL your queries specify the database.\r\n\r\n### To run Pig tests in Validatar:\r\n\r\n    export HADOOP_CLASSPATH=\"$HADOOP_CLASSPATH:/path/to/pig/lib/*\" (Add other jars here depending on your pig exec type or if hive/hcat is used in Pig)\r\n    hadoop jar validatar-jar-with-dependencies.jar com.yahoo.validatar.App -s tests/ --report report.xml --pig-exec-type mr --pig-setting 'mapreduce.job.acl-view-job=*' ...\r\n\r\nPig parameters are not supported in the pig query. Instead, use our parameter substitution (see below).\r\n\r\nRunning REST tests require no other dependencies and can be launched with Java instead of hadoop jar.\r\n\r\n## Help\r\n\r\nFeel free to reach out to us if you run into issues. You are welcome to open any issues. Pull requests welcome!\r\n\r\nWe list the complete help output from Validatar for reference here:\r\n```\r\nApplication options:\r\nOption (* = required)             Description\r\n---------------------             -----------\r\n-h, --help                        Shows help message.\r\n--parameter <Parameter>           Parameter to replace all '${VAR}' in\r\n                                    the query string. Ex: --parameter\r\n                                    DATE=2014-07-24\r\n* --test-suite <File: Test suite  File or folder that contains the test\r\n  file/folder>                      suite file(s).\r\n\r\n\r\nHive engine options:\r\nOption (* = required)                   Description\r\n---------------------                   -----------\r\n--hive-driver <Hive driver>             Fully qualified package name to the\r\n                                          hive driver. (default: org.apache.\r\n                                          hive.jdbc.HiveDriver)\r\n* --hive-jdbc <Hive JDBC connector>     JDBC string to the HiveServer2 with an\r\n                                          optional database. If the database\r\n                                          is provided, the queries must NOT\r\n                                          have one. Ex: 'jdbc:hive2:\r\n                                          //HIVE_SERVER:PORT/\r\n                                          [DATABASE_FOR_ALL_QUERIES]'\r\n--hive-password <Hive server password>  Hive server password. (default: anon)\r\n--hive-setting <Hive generic settings   Settings and their values. Ex: 'hive.\r\n  to use.>                                execution.engine=mr'\r\n--hive-username <Hive server username>  Hive server username. (default: anon)\r\n\r\nREST Engine options:\r\nOption                               Description\r\n------                               -----------\r\n--rest-function <REST Javascript     The name of the Javascript function\r\n  method name>                         used in all queries (default:\r\n                                       process)\r\n--rest-retry <Integer: REST Query    The default number of times to retry\r\n  retry limit>                         each HTTP request (default: 3)\r\n--rest-timeout <Integer: REST Query  The default time to wait for each HTTP\r\n  timeout>                             request (default: 60000)\r\n\r\nThis REST Engine works by making a HTTP GET or POST, parsing the response (JSON is best)\r\nusing your provided native JavaScript into a common format.\r\nThe query part of the engine is a JavaScript function that takes your response from your\r\nrequest and transforms it to a columnar JSON object with the columns as keys and values\r\nas arrays of values. You may need to iterate over your output and pull out your columns\r\nand return it as a JSON string using JSON stringify. Example: Suppose you extracted\r\ncolumns called 'a' and 'b', you would create and return the following JSON string :\r\n{\"a\": [a1, a2, ... an], \"b\": [b1, b2, ... bn]}\r\nThis engine will inspect these elements and convert them to the proper typed objects.\r\nThe metadata part of the query contains the required key/value pairs for making the REST\r\ncall. The url to make the request to can be set using the url. You can use a\r\ncustom timeout in ms for the call using rest-timeout. The HTTP method can be set\r\nusing the method - currently support GET and POST\r\nThe string body for the POST can be set using the body. The number of\r\ntimes to retry can be set using rest-retry. If you wish to change the name of the\r\nJavascript function you are using, use the rest-function. Default name is\r\nprocess. Any other key/value pair is added as headers to the REST call,\r\nwith the key being the header name and the value, its value.\r\n\r\nPig engine options:\r\nOption                                  Description\r\n------                                  -----------\r\n--pig-exec-type <Pig execution type>    The exec-type for Pig to use.  This is\r\n                                          the -x argument used when running\r\n                                          Pig. Ex: local, mr, tez etc.\r\n                                          (default: mr)\r\n--pig-output-alias <Pig default output  The default name of the alias where\r\n  alias>                                  the result is.This should contain\r\n                                          the data that will be collected\r\n                                          (default: validatar_results)\r\n--pig-setting <Pig generic settings to  Settings and their values. The -D\r\n  use.>                                   params that would have been sent to\r\n                                          Pig. Ex: 'mapreduce.job.acl-view-\r\n                                          job=*'\r\n\r\n\r\nReporting options:\r\nOption                           Description\r\n------                           -----------\r\n--report-format <Report format>  Which report format to use. (default:\r\n                                   junit)\r\n\r\n\r\nJunit report options:\r\nOption                       Description\r\n------                       -----------\r\n--report-file <Report file>  File to store the test reports.\r\n                               (default: report.xml)\r\n```\r\n\r\n## Changelog\r\n\r\nVersion | Notes\r\n------- | -----\r\n0.1.4 | Initial release with Hive\r\n0.1.5 | Typesystem, metadata support\r\n0.1.6 | No feature release. Source and Javadoc bundled in artifact\r\n0.1.7 | Multiple Hive databases across Queries\r\n0.1.8 | Null types in Hive results fix\r\n0.1.9 | Empty results handling bug fix\r\n0.2.0 | Internal switch to Java 8. hive-queue is no longer a setting. Use hive-setting.\r\n0.3.0 | Pig support added.\r\n0.4.0 | Rest API datasource added.\r\n\r\n## Members\r\n\r\nAkshai Sarma, akshaisarma@gmail.com\r\nJosh Walters, josh@joshwalters.com\r\n\r\n## Contributors\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}